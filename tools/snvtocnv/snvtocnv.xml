<tool id="snvtocnv" name="Infer CNVs from SNVs" version="0.2.2">
    <description>
    </description>
    <macros>
        <import>macro.xml</import>
    </macros>
    <requirements>
        <requirement type="package" version="1.6">samtools</requirement>
        <requirement type="package" version="3.0.0">sequenza-utils</requirement>
        <requirement type="package" version="3.0.0">r-sequenza</requirement>
        <requirement type="package" version="1.6.6">r-optparse</requirement>
        <requirement type="package" version="1.24.0">bioconductor-biocparallel</requirement>
        <requirement type="package" version="1.3.0">r-tidyverse</requirement>
    </requirements>
    <stdio>
        <exit_code range="1:" level="fatal" description="Error occured" />
    </stdio>
    <command detect_errors="exit_code"><![CDATA[
    @pipefail@
    @set_fasta_index@
    #if $sanitize == 'yes':
        python $__tool_directory__/sanitize_snv.py $input_snvs input_snv.vcf &&
    #else:
        ln -s $input_snvs input_snv.vcf &&
    #end if
    sequenza-utils gc_wiggle  --fasta reference.fa -o $refwig -w 50 &&
    sequenza-utils snp2seqz -v '$input_snvs'  -gc $refwig -o sample.seqz.gz &&
    sequenza-utils seqz_binning --seqz sample.seqz.gz -w 50 -o '$wiggle' &&
    Rscript $__tool_directory__/segmentation_sequenza.R
            -i '$wiggle'
            -s sample
            -O test &&
    Rscript $__tool_directory__/sequenza_to_hrdtools_input.R
            -i test/sample_segments.txt
            -s test/sample_alternative_solutions.txt
            -o '$cnvs'
    ]]></command>
    <inputs>
        <expand macro="reference_source_conditional" />
        <param name="input_snvs" type="data" format="vcf" label="SNVs to process in a vcf file"/>
        <param name="sanitize" type="select" label="Sanitize the vcf input"
               help="remove haplotype chromosomes and chr prefix">
            <option value="no" selected="true">NO</option>
            <option value="yes">YES</option>
        </param>
    </inputs>
    <outputs>
        <data name="refwig" format="txt" label="reference_wig" />
        <data name="wiggle" format="txt" label="binned wiggle" />
        <data name="sample_segment" format="txt" label="sample segments" from_work_dir="test/sample_segments.txt" />
        <data name="alt_solutions" format="txt" label="alternate solutions" from_work_dir="test/sample_alternative_solutions.txt" />
        <data name="cnvs" format="tabular" label="Annotated CNVs" />
    </outputs>
    <tests>
        <test>
            <param name="input_snvs" value="hg19_chr22.vcf" ftype="vcf" />
            <param name="reference_source_selector" value="history" />
            <param name="ref_file" value="hg19_chr22.fa.gz" />
            <output name="cnvs" file="hg19.cnv.tab" ftype="tabular" />
        </test>
        <test>
            <param name="input_snvs" value="hg38_chr22.vcf" ftype="vcf" />
            <param name="reference_source_selector" value="history" />
            <param name="ref_file" value="hg38_chr22.fa.gz" />
            <output name="cnvs" file="hg38.cnv.tab" ftype="tabular" />
        </test>
    </tests>
    <help>

snvtocnv
============================

This tool is wrapping several cleaning steps to produce bam files suitable for subsequent
analyses with lumpy-smoove (or other large structural variation callers) or with
somatic-varscan (or small structural variation callers)


Workflow 
=============

The tool is using the following command line for filtering:

::

    sambamba view -h -t 8 --filter='mapping_quality >= 1 and not(unmapped) and not(mate_is_unmapped)' -f 'bam' $input_base".bam"
    &#124; samtools rmdup - -
    &#124;tee $input_base".filt1.dedup.bam" &#124; bamleftalign --fasta-reference reference.fa -c --max-iterations "5" -
    &#124; samtools calmd  -C 50 -b -@ 4 - reference.fa &gt; $input_base".filt1.dedup.bamleft.calmd.bam" ;
    sambamba view -h -t 8 --filter='mapping_quality &lt;&#61; 254' -f 'bam' -o $input_base".filt1.dedup.bamleft.calmd.filt2.bam" $input_base".filt1.dedup.bamleft.calmd.bam"
    
Purpose
--------

This "workflow" tool was generated in order to limit the number of ``python metadata/set.py`` jobs
which occur at each step of standard galaxy workflows. Indeed, these jobs are poorly optimized and may last considerable
amounts of time when datasets are large, at each step, lowering the overall performance of the workflow.

    </help>
    <citations>
        <citation type="doi">10.1371/journal.pone.0168397</citation>
    </citations>
</tool>
